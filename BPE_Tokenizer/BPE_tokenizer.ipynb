{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author : Clement Ong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# basic pre-processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "import numpy as np  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_words = []\n",
    "result_vocab = []\n",
    "result_merge = []\n",
    "toy_vocab = {'newest':6, 'low':5, 'widest':3, 'lower':2, 'longer':1}\n",
    "toy_result_vocab = ['longer_', 'newest_', 'widest_', 'ewest_', 'idest_', 'lower_', 'dest_', 'est_', 'ger_', 'low_', 'er_', 'lon', 'low', 'es', 'ew', 'lo', 'r_', 't_', 'w_',\n",
    "                    'd', 'e', 'g', 'i', 'l', 'n', 'o', 's', 'w']\n",
    "toy_result_merge = ['e s', 'es t_', 'l o', 'e w', 'ew est_', 'n ewest_', 'lo w_', 'd est_', 'e r_', 'i dest_', 'w idest_',\n",
    "                    'lo w', 'low er_', 'g er_', 'lo n', 'lon ger_']\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(review):\n",
    "    review = [stemmer.stem(w.lower()) for w in word_tokenize(re.sub('[^a-zA-Z]+', ' ', review.replace(\"<br />\", \"\"))) if not w in stop_words]\n",
    "    return review\n",
    "\n",
    "def sort_bpe_vocab(vocab):\n",
    "    sorted_vocab = sorted(list(vocab))\n",
    "    sorted_vocab = sorted(list(sorted_vocab), key=len, reverse=True)\n",
    "    return sorted_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(vocab_dict):\n",
    "    \"\"\" Separate each char in word by space and add mark end of token, returning the unique list token\"\"\"\n",
    "    tokens = [\" \".join(word) + \"_\" for word in vocab_dict.keys()] \n",
    "    v_values = list(vocab_dict.values())\n",
    "    vocab_dict = dict(zip(tokens, v_values))\n",
    "\n",
    "    last_char_list = [w[-2:] for w in tokens]\n",
    "    char_list = [ i for w in tokens for i in w[:-2] if i != ' ']\n",
    "    unique_tokens = set(char_list + last_char_list)\n",
    "    \n",
    "    return vocab_dict, list(unique_tokens)\n",
    "\n",
    "\n",
    "def get_stats(vocab_dict):\n",
    "    \"\"\"Retrieve pair frequency\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, frequency in vocab_dict.items():\n",
    "        symbols = word.split(' ')\n",
    "        # Counting up occurrences of pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += frequency\n",
    "    return pairs\n",
    "\n",
    "def find_adjacent_char_merge(adj_vocab_dict):\n",
    "      \"\"\"Given {('n', 'e'): 2, ('e', 'w'): 2,('w', 'e'): 1,} find alphabet to be sorted\"\"\"\n",
    "      max_count = np.max(np.array(list(adj_vocab_dict.values())))\n",
    "      max_index = np.where(np.array(list(adj_vocab_dict.values())) == max_count)[0]\n",
    "      if len(max_index) > 1:\n",
    "            keys_dict = list(itemgetter(*max_index)(list(adj_vocab_dict.keys())))\n",
    "            keys_dict = sort_bpe_vocab(keys_dict)\n",
    "            return keys_dict[0]\n",
    "      else:\n",
    "            return list(adj_vocab_dict.keys())[max_index[0]]\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"Merging Frequent Pairs\"\"\"\n",
    "    vocab_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        # replace freq vocab with pair\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        vocab_out[w_out] = v_in[word]\n",
    "    return vocab_out\n",
    "\n",
    "def learn_bpe(vocab, num_vocab):\n",
    "    result_vocab = []\n",
    "    result_merge = []\n",
    "    vc_dict, result_vocab = build_vocab(vocab)\n",
    "    while len(result_vocab) < num_vocab:\n",
    "        pair_frequency = get_stats(vc_dict)\n",
    "        if not pair_frequency:\n",
    "            break\n",
    "        pair = find_adjacent_char_merge(pair_frequency)\n",
    "        vc_dict = merge_vocab(pair, vc_dict)\n",
    "        result_vocab.append(\"\".join(pair))\n",
    "        result_merge.append(\" \".join(pair))\n",
    "    return sort_bpe_vocab(result_vocab), result_merge\n",
    "\n",
    "result_vocab, result_merge = learn_bpe(vocab=toy_vocab, num_vocab=5000)\n",
    "len(result_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe tokenizer based on learned vocab\n",
    "def bpe_tokenize(sentence, bpe_merges, bpe_vocab):\n",
    "    words = [stemmer.stem(w.lower()) for w in word_tokenize(re.sub('[^a-zA-Z]+', ' ', sentence.replace(\"<br />\", \"\"))) if not w in stop_words]\n",
    "    result_tokens = []\n",
    "    for w in words: \n",
    "        w = w.replace('', ' ')[1:-1]\n",
    "        w += \"_\"\n",
    "        for pair in bpe_merges:\n",
    "            replacement = pair.replace(\" \", \"\")\n",
    "            pattern = r'(?:(?<=\\s)|(?<=^))'+ re.escape(pair)\n",
    "            w = re.sub(pattern, replacement, w)\n",
    "        sym_vocab = []\n",
    "        vocab_found = False\n",
    "        for sym in w.split():\n",
    "            if (sym not in bpe_vocab):\n",
    "                sym_vocab.append(\"UNK\")\n",
    "            else:\n",
    "                sym_vocab.append(sym)\n",
    "                vocab_found = True\n",
    "        if (not vocab_found):\n",
    "            result_tokens.append(\"UNK\")\n",
    "        else : \n",
    "            result_tokens.extend(sym_vocab)\n",
    "    return result_tokens\n",
    "\n",
    "print(bpe_tokenize('fewest gewest makes me new clement', result_merge, result_vocab))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
