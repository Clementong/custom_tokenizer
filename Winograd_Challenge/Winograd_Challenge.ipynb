{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.optimization import AdamW\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR = 'data/'\n",
    "MODEL_DIR = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_MC(input_filepath, output_filepath): \n",
    "    \"\"\"\n",
    "    Convert the given dataset into the output format with more features such as 'ending0', 'startphrase', ... \n",
    "    \"\"\"\n",
    "    with open(input_filepath, 'r', encoding=\"utf-8\") as ip: \n",
    "        for row in ip: \n",
    "            data = json.loads(row)\n",
    "            sent = data['sentence']\n",
    "            sent_part1, sent_part2 = sent.split(\"_\")\n",
    "            option1_sent_part2 = data['option1'] + sent_part2\n",
    "            option2_sent_part2 = data['option2'] + sent_part2\n",
    "            sentence = {'qID':data['qID'], 'sent1': sent_part1, 'sent2': '', 'startphrase': sent_part1, 'ending0': option1_sent_part2, 'ending1': option2_sent_part2}\n",
    "            ans = data.get('answer')\n",
    "            if (ans):\n",
    "                sentence['label'] = int(data['answer']) - 1\n",
    "            else: \n",
    "                sentence['label'] = int(data['qID'].split(\"-\")[-1]) - 1\n",
    "            with open(output_filepath, 'a', encoding=\"utf-8\") as op:\n",
    "                op.write(json.dumps(sentence))\n",
    "                op.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['train', 'test', 'dev']\n",
    "for d in data: \n",
    "    input_file = f\"{FILE_DIR}/{d}.jsonl\" \n",
    "    output_file = f\"{FILE_DIR}/MC_converted_{d}.jsonl\" \n",
    "#     open (output_file, 'w', encoding='utf-8').close() # uncomment if you have an exiting file of the same name.\n",
    "    convert_dataset_MC(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for the preprocessing function\n",
    "ending_names = [\"ending0\", \"ending1\"]\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 2 for context in examples[\"sent1\"]]\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    # flatten the 2 combined examples \n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "      \n",
    "    return {k: [v[i : i + 2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}\n",
    "\n",
    "# Load dataset to get it ready for tokenization\n",
    "d = load_dataset('json', data_files={'train': f'{FILE_DIR}/MC_converted_train.jsonl', 'validation': f'{FILE_DIR}/MC_converted_dev.jsonl', 'test': f'{FILE_DIR}/MC_converted_test.jsonl'})\n",
    "# tokenize dataset using map \n",
    "tokenized_d = d.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features): \n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        \n",
    "        labels = [feature.pop(label_name) for feature in features] \n",
    "        batch_size = len(features) \n",
    "\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "         \n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64) \n",
    "      \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base pre-trained model\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Optimizer selected (AdamW)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    eps=1e-30,\n",
    "    weight_decay=0.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to the device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./results\",\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  learning_rate=5e-4,\n",
    "  per_device_train_batch_size=8,\n",
    "  per_device_eval_batch_size=8,\n",
    "  num_train_epochs=4,\n",
    "  weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_d[\"train\"],\n",
    "    eval_dataset=tokenized_d[\"validation\"], \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics = compute_metrics,\n",
    "    optimizers=(optimizer,None)\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test(trainer,data):\n",
    "    \"\"\"\n",
    "    Compute accuracy scores for the dataset\n",
    "    \"\"\"\n",
    "    predictions = trainer.predict(data)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)# get the raw score with the higher value as the prediction\n",
    "    metric = load_metric(\"accuracy\" , '')\n",
    "    return metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of pre-fine tuned model\n",
    "print (\"Train Dataset\", compute_test(trainer, tokenized_d[\"train\"]))\n",
    "print (\"Evaluation Dataset\", compute_test(trainer, tokenized_d[\"validation\"]))\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of post-fine tuned model\n",
    "print (\"Train Dataset\", compute_test(trainer, tokenized_d[\"train\"]))\n",
    "print (\"Evaluation Dataset\", compute_test(trainer, tokenized_d[\"validation\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
